{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>1) Data Load </H1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1.1  Import All Spark Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://3724181074fa:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1591271582616)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-04 11:53:00,150 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import spark.implicits._\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql.SQLContext\n",
       "import org.apache.spark.{SparkContext, SparkConf}\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.ml.feature.Imputer\n",
       "import org.apache.spark.sql._\n",
       "import SQLContext._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import  org.apache.spark._ \n",
    "import  spark.implicits._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.{SparkContext, SparkConf}\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.feature.Imputer\n",
    "import org.apache.spark.sql._\n",
    "import SQLContext._\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Read the Weather input file - this is stored in the local file directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weather_org_df: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 22 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weather_org_df = spark.\n",
    "        read.\n",
    "        option(\"inferSchema\", \"true\").\n",
    "        option(\"header\",\"true\").\n",
    "        option(\"mode\",\"DROPMALFORMED\").\n",
    "        option(\"delimiter\", \",\").\n",
    "        csv(\"weatherAUS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Check the Schema of the Data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- MinTemp: string (nullable = true)\n",
      " |-- MaxTemp: string (nullable = true)\n",
      " |-- Rainfall: string (nullable = true)\n",
      " |-- Evaporation: string (nullable = true)\n",
      " |-- Sunshine: string (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: string (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: string (nullable = true)\n",
      " |-- WindSpeed3pm: string (nullable = true)\n",
      " |-- Humidity9am: string (nullable = true)\n",
      " |-- Humidity3pm: string (nullable = true)\n",
      " |-- Pressure9am: string (nullable = true)\n",
      " |-- Pressure3pm: string (nullable = true)\n",
      " |-- Cloud9am: string (nullable = true)\n",
      " |-- Cloud3pm: string (nullable = true)\n",
      " |-- Temp9am: string (nullable = true)\n",
      " |-- Temp3pm: string (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RISK_MM: double (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_org_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 Transform the Dataframe columns to correct column type for effective use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- MinTemp: double (nullable = true)\n",
      " |-- MaxTemp: double (nullable = true)\n",
      " |-- Rainfall: double (nullable = true)\n",
      " |-- Evaporation: double (nullable = true)\n",
      " |-- Sunshine: double (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: double (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: double (nullable = true)\n",
      " |-- WindSpeed3pm: double (nullable = true)\n",
      " |-- Humidity9am: double (nullable = true)\n",
      " |-- Humidity3pm: double (nullable = true)\n",
      " |-- Pressure9am: double (nullable = true)\n",
      " |-- Pressure3pm: double (nullable = true)\n",
      " |-- Cloud9am: double (nullable = true)\n",
      " |-- Cloud3pm: double (nullable = true)\n",
      " |-- Temp9am: double (nullable = true)\n",
      " |-- Temp3pm: double (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RISK_MM: double (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "weather_org_df_cast: org.apache.spark.sql.DataFrame = [Date: date, Location: string ... 22 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weather_org_df_cast = weather_org_df.withColumn(\"Date\",to_date($\"Date\",\"dd/MM/yyyy\"))\n",
    "            .withColumn(\"MinTemp\",col(\"MinTemp\").cast(DoubleType))\n",
    "            .withColumn(\"MaxTemp\",col(\"MaxTemp\").cast(DoubleType))\n",
    "            .withColumn(\"Rainfall\",col(\"Rainfall\").cast(DoubleType))\n",
    "            .withColumn(\"Evaporation\",col(\"Evaporation\").cast(DoubleType))\n",
    "            .withColumn(\"Sunshine\",col(\"Sunshine\").cast(DoubleType))\n",
    "            .withColumn(\"WindGustSpeed\",col(\"WindGustSpeed\").cast(DoubleType))\n",
    "            .withColumn(\"WindSpeed9am\",col(\"WindSpeed9am\").cast(DoubleType))\n",
    "            .withColumn(\"WindSpeed3pm\",col(\"WindSpeed3pm\").cast(DoubleType))\n",
    "            .withColumn(\"Humidity9am\",col(\"Humidity9am\").cast(DoubleType))\n",
    "            .withColumn(\"Humidity3pm\",col(\"Humidity3pm\").cast(DoubleType))\n",
    "            .withColumn(\"Pressure9am\",col(\"Pressure9am\").cast(DoubleType))\n",
    "            .withColumn(\"Pressure3pm\",col(\"Pressure3pm\").cast(DoubleType))\n",
    "            .withColumn(\"Cloud9am\",col(\"Cloud9am\").cast(DoubleType))\n",
    "            .withColumn(\"Cloud3pm\",col(\"Cloud3pm\").cast(DoubleType))\n",
    "            .withColumn(\"Temp9am\",col(\"Temp9am\").cast(DoubleType))\n",
    "            .withColumn(\"Temp3pm\",col(\"Temp3pm\").cast(DoubleType))\n",
    "weather_org_df_cast.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5 Basic checks to make sure all records are loaded from source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Long = 142193\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_org_df_cast.count() // count of records for confirmation that all records are loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.6 Create table for easy data manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_org_df_cast.createOrReplaceTempView(\"Aus_Rain_Data\")  // Create table for easy manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+-------+------------+\n",
      "|      Date|Location|MinTemp|MaxTemp|Rainfall|Evaporation|Sunshine|WindGustDir|WindGustSpeed|WindDir9am|WindDir3pm|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|Cloud9am|Cloud3pm|Temp9am|Temp3pm|RainToday|RISK_MM|RainTomorrow|\n",
      "+----------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+-------+------------+\n",
      "|2008-12-01|  Albury|   13.4|   22.9|     0.6|       null|    null|          W|         44.0|         W|       WNW|        20.0|        24.0|       71.0|       22.0|     1007.7|     1007.1|     8.0|    null|   16.9|   21.8|       No|    0.0|          No|\n",
      "|2008-12-02|  Albury|    7.4|   25.1|     0.0|       null|    null|        WNW|         44.0|       NNW|       WSW|         4.0|        22.0|       44.0|       25.0|     1010.6|     1007.8|    null|    null|   17.2|   24.3|       No|    0.0|          No|\n",
      "|2008-12-03|  Albury|   12.9|   25.7|     0.0|       null|    null|        WSW|         46.0|         W|       WSW|        19.0|        26.0|       38.0|       30.0|     1007.6|     1008.7|    null|     2.0|   21.0|   23.2|       No|    0.0|          No|\n",
      "|2008-12-04|  Albury|    9.2|   28.0|     0.0|       null|    null|         NE|         24.0|        SE|         E|        11.0|         9.0|       45.0|       16.0|     1017.6|     1012.8|    null|    null|   18.1|   26.5|       No|    1.0|          No|\n",
      "|2008-12-05|  Albury|   17.5|   32.3|     1.0|       null|    null|          W|         41.0|       ENE|        NW|         7.0|        20.0|       82.0|       33.0|     1010.8|     1006.0|     7.0|     8.0|   17.8|   29.7|       No|    0.2|          No|\n",
      "+----------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from Aus_Rain_Data\").show(5) // First look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Weather stations Data collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weather_dist_Location: org.apache.spark.sql.DataFrame = [Location: string, count: bigint]\n",
       "res5: Long = 49\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weather_dist_Location = weather_org_df_cast.select($\"Location\").groupBy(\"Location\").count()\n",
    "weather_dist_Location.count() // Number of Locations data collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|        location|\n",
      "+----------------+\n",
      "|        Adelaide|\n",
      "|          Albany|\n",
      "|          Albury|\n",
      "|    AliceSprings|\n",
      "|   BadgerysCreek|\n",
      "|        Ballarat|\n",
      "|         Bendigo|\n",
      "|        Brisbane|\n",
      "|          Cairns|\n",
      "|        Canberra|\n",
      "|           Cobar|\n",
      "|    CoffsHarbour|\n",
      "|        Dartmoor|\n",
      "|          Darwin|\n",
      "|       GoldCoast|\n",
      "|          Hobart|\n",
      "|       Katherine|\n",
      "|      Launceston|\n",
      "|       Melbourne|\n",
      "|MelbourneAirport|\n",
      "|         Mildura|\n",
      "|           Moree|\n",
      "|    MountGambier|\n",
      "|     MountGinini|\n",
      "|       Newcastle|\n",
      "|            Nhil|\n",
      "|       NorahHead|\n",
      "|   NorfolkIsland|\n",
      "|       Nuriootpa|\n",
      "|      PearceRAAF|\n",
      "|         Penrith|\n",
      "|           Perth|\n",
      "|    PerthAirport|\n",
      "|        Portland|\n",
      "|        Richmond|\n",
      "|            Sale|\n",
      "|      SalmonGums|\n",
      "|          Sydney|\n",
      "|   SydneyAirport|\n",
      "|      Townsville|\n",
      "|     Tuggeranong|\n",
      "|           Uluru|\n",
      "|      WaggaWagga|\n",
      "|         Walpole|\n",
      "|        Watsonia|\n",
      "|     Williamtown|\n",
      "|     Witchcliffe|\n",
      "|      Wollongong|\n",
      "|         Woomera|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT distinct location from Aus_Rain_Data order by Location\").show(49) // Check the location of the weather stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data is available in 49 weather stations </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Number of Years The records are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|First_record|\n",
      "+------------+\n",
      "|  2007-11-01|\n",
      "+------------+\n",
      "\n",
      "+-----------+\n",
      "|Last_Record|\n",
      "+-----------+\n",
      "| 2017-06-25|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"select date as First_record from Aus_Rain_Data order by date limit 1 \").show()\n",
    "spark.sql(\"select date as Last_Record from Aus_Rain_Data order by date desc limit 1 \").show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data is availble for from Nov 2007 to June 2017 - Nearly 10 years records . </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Data Range across Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+-----------------+------------------+-----------------+\n",
      "|summary|           MinTemp|          MaxTemp|          Temp9am|          Temp3pm|       Pressure9am|      Pressure3pm|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+------------------+-----------------+\n",
      "|  count|            141556|           141871|           141289|           139467|            128179|           128212|\n",
      "|   mean|12.186399728729263|23.22678419127248|16.98750858170129|21.68723497314778|1017.6537584159663|1015.258203537901|\n",
      "| stddev| 6.403282674671343|7.117618141018147|6.492838325478932|6.937593868533731|7.1054757115203016|7.036676783493396|\n",
      "|    min|              -8.5|             -4.8|             -7.2|             -5.4|             980.5|            977.1|\n",
      "|    max|              33.9|             48.1|             40.2|             46.7|            1041.0|           1039.6|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+------------------+-----------------+\n",
      "\n",
      "+-------+------------------+-----------------+-----------------+-----------------+------------------+------------------+\n",
      "|summary|     WindGustSpeed|     WindSpeed9am|     WindSpeed3pm|         Rainfall|       Evaporation|          Sunshine|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+------------------+------------------+\n",
      "|  count|            132923|           140845|           139563|           140787|             81350|             74377|\n",
      "|   mean| 39.98429165757619|  14.001988000994|18.63757586179718|2.349974074310995|5.4698242163490995| 7.624853113193616|\n",
      "| stddev|13.588800765487786|8.893337098234486|8.803345036235541|8.465172917616444| 4.188536508895148|3.7815249942144633|\n",
      "|    min|               6.0|              0.0|              0.0|              0.0|               0.0|               0.0|\n",
      "|    max|             135.0|            130.0|             87.0|            371.0|             145.0|              14.5|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+------------------+------------------+\n",
      "\n",
      "+-------+------------------+-----------------+------------------+------------------+\n",
      "|summary|          Cloud9am|         Cloud3pm|       Humidity9am|       Humidity3pm|\n",
      "+-------+------------------+-----------------+------------------+------------------+\n",
      "|  count|             88536|            85099|            140419|            138583|\n",
      "|   mean| 4.437189391885787|4.503166899728551|  68.8438103105705|51.482606091656265|\n",
      "| stddev|2.8870155257336068|2.720632530403662|19.051292535336284|20.797771843698907|\n",
      "|    min|               0.0|              0.0|               0.0|               0.0|\n",
      "|    max|               9.0|              9.0|             100.0|             100.0|\n",
      "+-------+------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_org_df_cast.select(\"MinTemp\",\"MaxTemp\",\"Temp9am\",\"Temp3pm\",\"Pressure9am\",\"Pressure3pm\").describe().show()\n",
    "weather_org_df_cast.select(\"WindGustSpeed\",\"WindSpeed9am\",\"WindSpeed3pm\",\"Rainfall\",\"Evaporation\",\"Sunshine\").describe().show()\n",
    "weather_org_df_cast.select(\"Cloud9am\",\"Cloud3pm\",\"Humidity9am\",\"Humidity3pm\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data across the entire population of data might give wrong interpretation . Lets look at by location as Australia is very big country and weather across different States vary too much</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 Data Range across Different Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-04 11:53:55,677 WARN  [Thread-4] util.Utils (Logging.scala:logWarning(66)) - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "+----------------+------------------+------------------+------------------+------------------+\n",
      "|        Location|        MinTempavg|        MaxTempavg|             Rfavg|         WindGSavg|\n",
      "+----------------+------------------+------------------+------------------+------------------+\n",
      "|        Adelaide|12.628367875647646|22.945401554404082| 1.572185430463575| 36.53081186827519|\n",
      "|          Albany|12.948461278322574|20.072587131367285| 2.255073431241649|              null|\n",
      "|          Albury|  9.52089850249585|22.630963455149466| 1.925710464727508| 32.95301566144619|\n",
      "|    AliceSprings|13.125181518151814|29.244191419141917|0.8693553719008265| 40.53371351895337|\n",
      "|   BadgerysCreek|11.136899862825791|24.023111111111124|2.2079249217935253| 33.60989010989011|\n",
      "|        Ballarat| 7.355302279484644|18.274793524942158| 1.688829963539929|44.978695073235684|\n",
      "|         Bendigo|  8.59106495219255|21.616683151994728|1.6214521452145108|38.849283094364786|\n",
      "|        Brisbane| 16.41099841521393| 26.44152676591707|3.1605363984674373| 28.21137952956135|\n",
      "|          Cairns|21.199196518245742|29.544344042837974| 5.765317139001365| 38.06799057556378|\n",
      "|        Canberra| 6.827688250805757|20.980644216691047|1.7350382128159827|  40.0821740537043|\n",
      "|           Cobar| 13.12034182305631| 25.83408101774354|1.1292617449664417|36.728813559322035|\n",
      "|    CoffsHarbour|14.365774313791945|23.915575161180858|  5.05459183673471| 39.23219696969697|\n",
      "|        Dartmoor| 8.619816138917264|19.694427454977948| 2.148553929908119| 38.89690370874447|\n",
      "|          Darwin|23.210529614540917| 32.54097744360893| 5.094047619047625| 40.58235480799746|\n",
      "|       GoldCoast| 17.34149043303128|25.752970795568952|3.7289329685362627|42.472538860103626|\n",
      "|          Hobart| 9.079140526976161|17.866677125823653| 1.604272698711901| 47.53139791732408|\n",
      "|       Katherine| 20.53498349834985| 34.93903708523091|3.1355987055016175|  38.7583278902678|\n",
      "|      Launceston| 7.833818181818179|18.956231404958686|2.0122185430463535| 35.63087695898633|\n",
      "|       Melbourne|11.788126540673797| 20.86014790468364|1.8377719756309814| 45.42544403139199|\n",
      "|MelbourneAirport| 9.973944832170167|20.491857760053122|1.4519774011299402| 46.91917167668671|\n",
      "|         Mildura|  10.7339541070835| 24.84153641503151|0.9450249584026625| 37.17082917082917|\n",
      "|           Moree|12.786255259467062| 26.88686054660126|1.6032963095664614| 39.79942487419123|\n",
      "|    MountGambier| 8.827467811158806| 19.82812809508089|2.0873593646591515| 42.86909943086709|\n",
      "|     MountGinini|3.6511929824561404|11.777946768060843|3.2452414772727214| 46.18821362799263|\n",
      "|       Newcastle|13.740239790183637|24.098282647584846| 3.153021509047461|              null|\n",
      "|            Nhil| 8.992797960484381|22.398406628425754|0.9329073482428145|42.542437779195915|\n",
      "|       NorahHead|15.375196985268934|22.607900136798932| 3.382479224376734|42.215042735042736|\n",
      "|   NorfolkIsland|16.839959500506236| 21.79274628879893| 3.137567934782603| 42.63997262149213|\n",
      "|       Nuriootpa|  9.36643263122702|21.714657762938188|1.3813751668891827|40.660720296196565|\n",
      "|      PearceRAAF|12.386313868613142|26.271048798252004|1.6343537414965963| 43.44854881266491|\n",
      "|         Penrith|12.533649289099511| 24.73198380566811|2.1351425626932192| 31.79438620223199|\n",
      "|           Perth|12.922705919198231|25.034837092731838|1.9062950203570268|34.882998745294856|\n",
      "|    PerthAirport|  12.5803589232303| 25.53323363243602|1.7616483881688256| 44.20182247721903|\n",
      "|        Portland| 9.585537328423172|18.055626043405685| 2.531032171581759|42.058882907133246|\n",
      "|        Richmond|11.346070091868002|24.449813242784405|2.1525808671713578| 34.86035422343324|\n",
      "|            Sale| 8.561220406802276|20.271023674558162|1.5126671122994568| 42.47452896022331|\n",
      "|      SalmonGums| 9.302578018995934| 24.24920365977637| 1.032301938116285|39.761086283946376|\n",
      "|          Sydney|14.865056988602275|23.002338830584726|3.3302311618132627|41.761408083441985|\n",
      "|   SydneyAirport|14.894041278295612| 23.37890183028289| 2.995334888370546|47.222071767095464|\n",
      "|      Townsville|20.411250412405153|29.362973953181562|3.4886025768087214| 38.80405181002989|\n",
      "|     Tuggeranong| 7.245612278945606|20.777444110777438|2.1590402162892834| 35.18961474036851|\n",
      "|           Uluru|14.406982872200256|30.387442472057817|0.7073235685752333| 41.36923076923077|\n",
      "|      WaggaWagga| 9.562029569892472| 22.96129032258059|1.7060851926977643|36.566146540027134|\n",
      "|         Walpole|11.915710723192051|20.566820768136616|2.8773285198555856| 39.75305096913137|\n",
      "|        Watsonia|10.126227865018391|20.883527842614196| 1.847692307692302|38.057296932928885|\n",
      "|     Williamtown|12.820376175548573|24.147019607843177|3.5109848484848385| 41.75369458128079|\n",
      "|     Witchcliffe|10.756861413043486| 21.69942314217853|2.8996580027359733|39.980258679373726|\n",
      "|      Wollongong|  14.9490578734859| 21.47651006711409|3.5891267414203223| 45.69525731584258|\n",
      "|         Woomera| 13.31389819156064| 26.54213520749662|0.4899463806970519|44.080962059620596|\n",
      "+----------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "winp1: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@17781a6f\n",
       "winp2: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@21588061\n",
       "aggDF: Unit = ()\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Partition the data to get by location & Date\n",
    "\n",
    "val winp1 = Window.partitionBy(\"Location\").orderBy(col(\"Date\"))\n",
    "    weather_org_df.withColumn(\"row\",row_number.over(winp1))\n",
    "      .where($\"row\" === 1).drop(\"row\")\n",
    "\n",
    "val winp2 = Window.partitionBy(\"Location\")\n",
    "    val aggDF = weather_org_df.withColumn(\"row\",row_number.over(winp1))\n",
    "    .withColumn(\"MinTempavg\", avg(col(\"MinTemp\")).over(winp2))\n",
    "    .withColumn(\"MaxTempavg\", avg(col(\"MaxTemp\")).over(winp2))\n",
    "    .withColumn(\"Rfavg\", avg(col(\"Rainfall\")).over(winp2))\n",
    "    .withColumn(\"Rfavg\", avg(col(\"Rainfall\")).over(winp2))\n",
    "    .withColumn(\"WindGSavg\", avg(col(\"WindGustSpeed\")).over(winp2))\n",
    "    \n",
    "      .where(col(\"row\")===1).select(\"Location\",\"MinTempavg\",\"MaxTempavg\",\"Rfavg\",\"WindGSavg\")\n",
    "      .orderBy(\"Location\").show(49)  // show all Weather station locations\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>As expected, We can see that there is a wide difference in the Weather conditions across , for example In Tuggeranog station the Average Min Tem is 7.24 but the average in Darwin is 23.2 </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5 Check the Null / NA columns acorss the data Range to decide on the data wrangling requirement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+-------+------------+\n",
      "|Date|Location|MinTemp|MaxTemp|Rainfall|Evaporation|Sunshine|WindGustDir|WindGustSpeed|WindDir9am|WindDir3pm|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|Cloud9am|Cloud3pm|Temp9am|Temp3pm|RainToday|RISK_MM|RainTomorrow|\n",
      "+----+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+-------+------------+\n",
      "|   0|       0|    637|    322|    1406|      60843|   67816|          0|         9270|         0|         0|        1348|        2630|       1774|       3610|      14014|      13981|   53657|   57094|    904|   2726|        0|      0|           0|\n",
      "+----+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Checking null Values across all acolumns at once !!! \n",
    "\n",
    "weather_org_df_cast.select(weather_org_df_cast.columns.map(colName => {\n",
    "    count(when(col(colName).isNull, true) || when(col(colName)===\"\", true)) as s\"${colName}\"\n",
    "  }): _*)\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>There is significantly large missing data in Evaporation , Sunshine and Cloud Data set . In the next feature during the Feature selection for modeling we need to make note of this.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>3) Data Analysis using visulaisation   </H1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our team decided to use Python for this purpose . A separate Python files has been attached and screen shots of those are attached to the Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>4) Data Wrangling  </H1>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4.1 Remove Duplicates - Location for the given Date uniquely identify the records . Hence lets use that \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weather_org_df_W1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Date: date, Location: string ... 22 more fields]\n",
       "res10: Long = 142193\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Remove duplicates based on Date and Location\n",
    "\n",
    "val weather_org_df_W1 =  weather_org_df_cast.distinct()\n",
    "weather_org_df_W1.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weather_org_df_W2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Date: date, Location: string ... 22 more fields]\n",
       "res11: Long = 142193\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weather_org_df_W2 =  weather_org_df_cast.dropDuplicates(Seq(\"Date\", \"Location\"))\n",
    "weather_org_df_W2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2  Convert blank columns into uniform notation so that imputation can be applied as uniform. As the data set is a time series data , imputing value will value can be  based \n",
    "\n",
    "    a) Regression\n",
    "    b) Nearest Data set approach\n",
    "    c) Splines (using MARS technique)\n",
    "    d) Simplying imputing avaerage of that column (This wil not be correct as the weather data is wide spread across States)\n",
    "    \n",
    "    We have follwed nearest data apprach as it is one of the most suited \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: Long = 637\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_org_df_cast.filter($\"MinTemp\".isNull).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weather_org_df_W2_impute: org.apache.spark.sql.DataFrame = [Date: date, Location: string ... 22 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weather_org_df_W2_impute= weather_org_df_cast.withColumn(\"MinTemp\",when(col(\"MinTemp\").isNull,lead(\"MinTemp\",1).over(winp1)).otherwise(col(\"MinTemp\")))\n",
    "                              .withColumn(\"MaxTemp\",when(col(\"MaxTemp\").isNull,lead(\"MaxTemp\",1).over(winp1)).otherwise(col(\"MaxTemp\")))\n",
    "                             .withColumn(\"Temp9am\",when(col(\"Temp9am\").isNull,lead(\"Temp9am\",1).over(winp1)).otherwise(col(\"Temp9am\")))\n",
    "                             .withColumn(\"Temp3pm\",when(col(\"Temp3pm\").isNull,lead(\"Temp3pm\",1).over(winp1)).otherwise(col(\"Temp3pm\")))\n",
    "                              .withColumn(\"Cloud9am\",when(col(\"Cloud9am\").isNull,lead(\"Cloud9am\",1).over(winp1)).otherwise(col(\"Cloud9am\")))\n",
    "                            .withColumn(\"Cloud3pm\",when(col(\"Cloud3pm\").isNull,lead(\"Cloud3pm\",1).over(winp1)).otherwise(col(\"Cloud3pm\")))\n",
    "                            .withColumn(\"Cloud3pm\",when(col(\"Cloud3pm\").isNull,lead(\"Cloud3pm\",1).over(winp1)).otherwise(col(\"Cloud3pm\")))\n",
    "                            .withColumn(\"Humidity9am\",when(col(\"Humidity9am\").isNull,lead(\"Humidity9am\",1).over(winp1)).otherwise(col(\"Humidity9am\")))\n",
    "                            .withColumn(\"Humidity3pm\",when(col(\"Humidity3pm\").isNull,lead(\"Humidity3pm\",1).over(winp1)).otherwise(col(\"Humidity3pm\")))\n",
    "                            .withColumn(\"Pressure9am\",when(col(\"Pressure9am\").isNull,lead(\"Pressure9am\",1).over(winp1)).otherwise(col(\"Pressure9am\")))\n",
    "                             .withColumn(\"Pressure9am\",when(col(\"Pressure9am\").isNull,lead(\"Pressure9am\",1).over(winp1)).otherwise(col(\"Pressure9am\")))\n",
    "                            .withColumn(\"Pressure3pm\",when(col(\"Pressure3pm\").isNull,lead(\"Pressure3pm\",1).over(winp1)).otherwise(col(\"Pressure3pm\")))\n",
    "                             .withColumn(\"WindGustSpeed\",when(col(\"WindGustSpeed\").isNull,lead(\"WindGustSpeed\",1).over(winp1)).otherwise(col(\"WindGustSpeed\")))\n",
    "                            .withColumn(\"WindSpeed9am\",when(col(\"WindSpeed9am\").isNull,lead(\"WindSpeed9am\",1).over(winp1)).otherwise(col(\"WindSpeed9am\")))\n",
    "                             .withColumn(\"WindSpeed3pm\",when(col(\"WindSpeed3pm\").isNull,lead(\"WindSpeed3pm\",1).over(winp1)).otherwise(col(\"WindSpeed3pm\")))\n",
    "                             .withColumn(\"Rainfall\",when(col(\"Rainfall\").isNull,lead(\"Rainfall\",1).over(winp1)).otherwise(col(\"Rainfall\")))\n",
    "                             .withColumn(\"Evaporation\",when(col(\"Evaporation\").isNull,lead(\"Evaporation\",1).over(winp1)).otherwise(col(\"Evaporation\")))\n",
    "                             .withColumn(\"Sunshine\",when(col(\"Sunshine\").isNull,lead(\"Sunshine\",1).over(winp1)).otherwise(col(\"Sunshine\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_org_df_W2_impute.createOrReplaceTempView(\"Aus_Rain_Data1\")  // Create table for easy manipulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     164|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     637|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select COUNT(*) from Aus_Rain_Data1 where MinTemp IS NULL \").show()\n",
    "spark.sql(\"select COUNT(*) from Aus_Rain_Data where MinTemp IS NULL \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weather_org_df_W3_impute: org.apache.spark.sql.DataFrame = [Date: date, Location: string ... 22 more fields]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weather_org_df_W3_impute= weather_org_df_W2_impute.withColumn(\"MinTemp\",when(col(\"MinTemp\").isNull,lag(\"MinTemp\",1).over(winp1)).otherwise(col(\"MinTemp\")))\n",
    "                              .withColumn(\"MaxTemp\",when(col(\"MaxTemp\").isNull,lag(\"MaxTemp\",1).over(winp1)).otherwise(col(\"MaxTemp\")))\n",
    "                             .withColumn(\"Temp9am\",when(col(\"Temp9am\").isNull,lag(\"Temp9am\",1).over(winp1)).otherwise(col(\"Temp9am\")))\n",
    "                             .withColumn(\"Temp3pm\",when(col(\"Temp3pm\").isNull,lag(\"Temp3pm\",1).over(winp1)).otherwise(col(\"Temp3pm\")))\n",
    "                              .withColumn(\"Cloud9am\",when(col(\"Cloud9am\").isNull,lag(\"Cloud9am\",1).over(winp1)).otherwise(col(\"Cloud9am\")))\n",
    "                            .withColumn(\"Cloud3pm\",when(col(\"Cloud3pm\").isNull,lag(\"Cloud3pm\",1).over(winp1)).otherwise(col(\"Cloud3pm\")))\n",
    "                            .withColumn(\"Cloud3pm\",when(col(\"Cloud3pm\").isNull,lag(\"Cloud3pm\",1).over(winp1)).otherwise(col(\"Cloud3pm\")))\n",
    "                            .withColumn(\"Humidity9am\",when(col(\"Humidity9am\").isNull,lag(\"Humidity9am\",1).over(winp1)).otherwise(col(\"Humidity9am\")))\n",
    "                            .withColumn(\"Humidity3pm\",when(col(\"Humidity3pm\").isNull,lag(\"Humidity3pm\",1).over(winp1)).otherwise(col(\"Humidity3pm\")))\n",
    "                            .withColumn(\"Pressure9am\",when(col(\"Pressure9am\").isNull,lag(\"Pressure9am\",1).over(winp1)).otherwise(col(\"Pressure9am\")))\n",
    "                             .withColumn(\"Pressure9am\",when(col(\"Pressure9am\").isNull,lag(\"Pressure9am\",1).over(winp1)).otherwise(col(\"Pressure9am\")))\n",
    "                            .withColumn(\"Pressure3pm\",when(col(\"Pressure3pm\").isNull,lag(\"Pressure3pm\",1).over(winp1)).otherwise(col(\"Pressure3pm\")))\n",
    "                             .withColumn(\"WindGustSpeed\",when(col(\"WindGustSpeed\").isNull,lag(\"WindGustSpeed\",1).over(winp1)).otherwise(col(\"WindGustSpeed\")))\n",
    "                            .withColumn(\"WindSpeed9am\",when(col(\"WindSpeed9am\").isNull,lag(\"WindSpeed9am\",1).over(winp1)).otherwise(col(\"WindSpeed9am\")))\n",
    "                             .withColumn(\"WindSpeed3pm\",when(col(\"WindSpeed3pm\").isNull,lag(\"WindSpeed3pm\",1).over(winp1)).otherwise(col(\"WindSpeed3pm\")))\n",
    "                             .withColumn(\"Rainfall\",when(col(\"Rainfall\").isNull,lag(\"Rainfall\",1).over(winp1)).otherwise(col(\"Rainfall\")))\n",
    "                             .withColumn(\"Evaporation\",when(col(\"Evaporation\").isNull,lag(\"Evaporation\",1).over(winp1)).otherwise(col(\"Evaporation\")))\n",
    "                             .withColumn(\"Sunshine\",when(col(\"Sunshine\").isNull,lag(\"Sunshine\",1).over(winp1)).otherwise(col(\"Sunshine\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_org_df_W3_impute.createOrReplaceTempView(\"Aus_Rain_Data2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     102|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select COUNT(*) from Aus_Rain_Data2 where MinTemp IS NULL \").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> For Remaining Null Values, Lets impute using the Monthly average for that location</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weather_org_df_W4_impute: org.apache.spark.sql.DataFrame = [Date: date, Location: string ... 23 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// First lets create a Month column to partition the records by Month and Location\n",
    "\n",
    "val weather_org_df_W4_impute = weather_org_df_W3_impute.withColumn(\"Month\",month(col(\"Date\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- MinTemp: double (nullable = true)\n",
      " |-- MaxTemp: double (nullable = true)\n",
      " |-- Rainfall: double (nullable = true)\n",
      " |-- Evaporation: double (nullable = true)\n",
      " |-- Sunshine: double (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: double (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: double (nullable = true)\n",
      " |-- WindSpeed3pm: double (nullable = true)\n",
      " |-- Humidity9am: double (nullable = true)\n",
      " |-- Humidity3pm: double (nullable = true)\n",
      " |-- Pressure9am: double (nullable = true)\n",
      " |-- Pressure3pm: double (nullable = true)\n",
      " |-- Cloud9am: double (nullable = true)\n",
      " |-- Cloud3pm: double (nullable = true)\n",
      " |-- Temp9am: double (nullable = true)\n",
      " |-- Temp3pm: double (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RISK_MM: double (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_org_df_W4_impute.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Month Column is added to the data frame </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets check the Monthly Averages before we impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "winp3: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@cd18bdd\n",
       "winp4: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@5d2432f5\n",
       "aggDF: org.apache.spark.sql.DataFrame = [Month: int, Location: string ... 16 more fields]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val winp3 = Window.partitionBy(\"Location\",\"Month\").orderBy(col(\"Date\"))\n",
    "    weather_org_df_W4_impute.withColumn(\"row\",row_number.over(winp3))\n",
    "      .where($\"row\" === 1).drop(\"row\")\n",
    "\n",
    "val winp4 = Window.partitionBy(\"Location\",\"Month\")\n",
    "    val aggDF = weather_org_df_W4_impute.withColumn(\"row\",row_number.over(winp3))\n",
    "    .withColumn(\"MinTempavg\", avg(col(\"MinTemp\")).over(winp4))\n",
    "    .withColumn(\"MaxTempavg\", avg(col(\"MaxTemp\")).over(winp4))\n",
    "    .withColumn(\"Rfavg\", avg(col(\"Rainfall\")).over(winp4))\n",
    "    .withColumn(\"Evapavg\", avg(col(\"Evaporation\")).over(winp4))\n",
    "    .withColumn(\"Sunavg\", avg(col(\"Sunshine\")).over(winp4))\n",
    "    .withColumn(\"WindGusavg\", avg(col(\"WindGustSpeed\")).over(winp4))\n",
    "    .withColumn(\"WindS9amavg\", avg(col(\"WindSpeed9am\")).over(winp4))\n",
    "    .withColumn(\"WindS3pmavg\", avg(col(\"WindSpeed3pm\")).over(winp4))\n",
    "    .withColumn(\"Hum9amavg\", avg(col(\"Humidity9am\")).over(winp4))\n",
    "    .withColumn(\"Hum3pmavg\", avg(col(\"Humidity3pm\")).over(winp4))\n",
    "    .withColumn(\"Pres9amavg\", avg(col(\"Pressure9am\")).over(winp4))\n",
    "    .withColumn(\"Pres3pmavg\", avg(col(\"Pressure3pm\")).over(winp4))\n",
    "    .withColumn(\"Clo9amavg\", avg(col(\"Cloud9am\")).over(winp4))\n",
    "    .withColumn(\"Clo9pmavg\", avg(col(\"Cloud3pm\")).over(winp4))\n",
    "    .withColumn(\"Tem9amavg\", avg(col(\"Temp9am\")).over(winp4))\n",
    "    .withColumn(\"Tem3pmavg\", avg(col(\"Temp3pm\")).over(winp4))\n",
    "    \n",
    "      .where(col(\"row\")===1).select(\"Month\",\"Location\",\"MinTempavg\",\"MaxTempavg\",\"Rfavg\",\"Evapavg\",\"Sunavg\",\"WindGusavg\",\"WindS9amavg\",\"WindS3pmavg\",\"Hum9amavg\",\n",
    "                                    \"Hum3pmavg\",\"Pressure9am\",\"Pressure3pm\",\"Cloud9am\",\"Cloud3pm\",\"Temp9am\",\"Temp3pm\")\n",
    "      .orderBy(\"Location\",\"Month\").toDF()  // "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the averages before impute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggDF.createOrReplaceTempView(\"Aus_Rain_Data_testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------+-----------+--------+--------+-------+-------+\n",
      "|Month|Location|        MinTempavg|        MaxTempavg|             Rfavg|           Evapavg|            Sunavg|        WindGusavg|       WindS9amavg|       WindS3pmavg|         Hum9amavg|         Hum3pmavg|Pressure9am|Pressure3pm|Cloud9am|Cloud3pm|Temp9am|Temp3pm|\n",
      "+-----+--------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------+-----------+--------+--------+-------+-------+\n",
      "|    1|  Sydney|20.316487455197134|27.517204301075285|3.1275985663082424| 7.705017921146956|7.5673835125448035|43.903225806451616| 12.46594982078853|21.205035971223023| 67.59498207885305|58.473118279569896|     1005.5|     1000.7|     4.0|     6.0|   22.0|   34.3|\n",
      "|    2|  Sydney| 20.18627450980393|26.880784313725492| 4.323921568627452| 6.666666666666669| 6.910980392156863| 40.84117647058824|12.854901960784314| 20.67058823529412| 72.48627450980392|60.819607843137256|     1017.6|     1017.4|     8.0|     8.0|   20.7|   20.9|\n",
      "|    3|  Sydney|18.857419354838704|25.881290322580632| 4.483870967741938|  5.67774193548387| 6.801290322580646| 41.86175115207373|12.974193548387097|20.258064516129032| 73.43548387096774|              59.8|     1020.9|     1020.6|     0.0|     1.0|   15.9|   21.2|\n",
      "|    4|  Sydney| 15.39444444444444|23.504444444444424|5.1948148148148166| 4.164552238805971| 6.767037037037038|              37.3| 15.22962962962963| 17.88888888888889| 72.67037037037036|57.733333333333334|     1023.0|     1021.5|     1.0|     1.0|   16.7|   23.3|\n",
      "|    5|  Sydney|12.110645161290318|21.153548387096773| 2.434193548387098| 3.328387096774194|6.9558064516129035| 36.97235023041475| 17.45098039215686|16.270358306188925| 71.14516129032258| 52.05483870967742|     1019.7|     1016.7|     7.0|     7.0|   13.4|   18.1|\n",
      "|    6|  Sydney| 10.50340136054422|18.373129251700686| 5.831972789115647|2.6394557823129263|5.3901360544217685| 39.80975609756098|17.401360544217688|15.642857142857142| 75.67687074829932| 57.53741496598639|     1029.3|     1027.0|     7.0|     7.0|   14.3|   18.7|\n",
      "|    7|  Sydney| 8.986738351254484|17.991397849462366|2.8530465949820805|2.8322580645161293| 6.681003584229389| 40.95108695652174| 17.35483870967742| 16.92831541218638| 71.44086021505376|49.494623655913976|     1011.0|     1008.5|     0.0|     1.0|   13.1|   19.2|\n",
      "|    8|  Sydney| 9.729749103942655| 19.34193548387096|2.1562724014336907|3.8971326164874527| 7.836559139784943| 40.95161290322581|17.491039426523297|18.053763440860216|62.064981949458485| 46.13309352517986|     1004.8|     1003.3|     6.0|     7.0|   17.2|   16.8|\n",
      "|    9|  Sydney|12.602230483271375|21.994052044609674|1.8178438661710021| 5.211524163568774|  8.24609665427509|43.642458100558656|            15.625|21.049056603773586| 57.54646840148699|48.427509293680295|     1022.9|     1020.8|     0.0|     0.0|   15.0|   19.0|\n",
      "|   10|  Sydney|14.769675090252706|23.460649819494588|2.1761732851985554| 6.225270758122739| 7.860288808664262| 45.07655502392345|14.552346570397113|21.216606498194945| 62.03249097472924| 51.04693140794224|     1020.9|     1015.7|     6.0|     1.0|   19.0|   20.5|\n",
      "|   11|  Sydney|17.347191011235953|  24.9625468164794|2.9101123595505625| 7.037453183520599| 7.476404494382018| 44.68627450980392|13.363295880149813|21.674157303370787| 64.77153558052434|57.449438202247194|     1021.9|     1022.9|     7.0|     7.0|   18.9|   20.4|\n",
      "|   12|  Sydney|18.605241935483864|25.861693548387112|2.3919354838709683| 7.481451612903226| 7.828225806451615| 44.95161290322581|13.608870967741936|22.173387096774192| 65.84274193548387| 57.84274193548387|     1009.1|     1004.6|     3.0|     7.0|   24.9|   30.8|\n",
      "+-----+--------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------+-----------+--------+--------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from Aus_Rain_Data_testing where Location = 'Sydney'\" ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Averages are calculated correctly for each month "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "winp5: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@7f234df9\n",
       "winp6: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@760ee998\n",
       "weather_org_df_ff: org.apache.spark.sql.DataFrame = [Date: date, Month: int ... 23 more fields]\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val winp5 = Window.partitionBy(\"Location\",\"Month\").orderBy(col(\"Date\"))\n",
    "    weather_org_df_W4_impute.withColumn(\"row\",row_number.over(winp3))\n",
    "      .where($\"row\" === 1).drop(\"row\")\n",
    "val winp6 = Window.partitionBy(\"Location\",\"Month\")\n",
    "\n",
    "val weather_org_df_ff= weather_org_df_W4_impute.withColumn(\"row\",row_number.over(winp5))\n",
    ".withColumn(\"MinTemp\",when(col(\"MinTemp\").isNull,avg(col(\"MinTemp\")).over(winp6)).otherwise(col(\"MinTemp\")))\n",
    ".withColumn(\"MaxTemp\",when(col(\"MaxTemp\").isNull,avg(col(\"MaxTemp\")).over(winp6)).otherwise(col(\"MaxTemp\")))\n",
    ".withColumn(\"Temp9am\",when(col(\"Temp9am\").isNull,avg(col(\"Temp9am\")).over(winp6)).otherwise(col(\"Temp9am\")))\n",
    ".withColumn(\"Temp3pm\",when(col(\"Temp3pm\").isNull,avg(col(\"Temp3pm\")).over(winp6)).otherwise(col(\"Temp3pm\")))\n",
    ".withColumn(\"Cloud9am\",when(col(\"Cloud9am\").isNull,avg(col(\"Cloud9am\")).over(winp6)).otherwise(col(\"Cloud9am\")))\n",
    ".withColumn(\"Cloud3pm\",when(col(\"Cloud3pm\").isNull,avg(col(\"Cloud3pm\")).over(winp6)).otherwise(col(\"Cloud3pm\")))\n",
    ".withColumn(\"Cloud3pm\",when(col(\"Cloud3pm\").isNull,avg(col(\"Cloud3pm\")).over(winp6)).otherwise(col(\"Cloud3pm\")))\n",
    ".withColumn(\"Humidity9am\",when(col(\"Humidity9am\").isNull,avg(col(\"Humidity9am\")).over(winp6)).otherwise(col(\"Humidity9am\")))\n",
    ".withColumn(\"Humidity3pm\",when(col(\"Humidity3pm\").isNull,avg(col(\"Humidity3pm\")).over(winp6)).otherwise(col(\"Humidity3pm\")))\n",
    ".withColumn(\"Pressure9am\",when(col(\"Pressure9am\").isNull,avg(col(\"Pressure9am\")).over(winp6)).otherwise(col(\"Pressure9am\")))\n",
    ".withColumn(\"Pressure9am\",when(col(\"Pressure9am\").isNull,avg(col(\"Pressure9am\")).over(winp6)).otherwise(col(\"Pressure9am\")))\n",
    ".withColumn(\"Pressure3pm\",when(col(\"Pressure3pm\").isNull,avg(col(\"Pressure3pm\")).over(winp6)).otherwise(col(\"Pressure3pm\")))\n",
    ".withColumn(\"WindGustSpeed\",when(col(\"WindGustSpeed\").isNull,avg(col(\"WindGustSpeed\")).over(winp6)).otherwise(col(\"WindGustSpeed\")))\n",
    ".withColumn(\"WindSpeed9am\",when(col(\"WindSpeed9am\").isNull,avg(col(\"WindSpeed9am\")).over(winp6)).otherwise(col(\"WindSpeed9am\")))\n",
    ".withColumn(\"WindSpeed3pm\",when(col(\"WindSpeed3pm\").isNull,avg(col(\"WindSpeed3pm\")).over(winp6)).otherwise(col(\"WindSpeed3pm\")))\n",
    ".withColumn(\"Rainfall\",when(col(\"Rainfall\").isNull,avg(col(\"Rainfall\")).over(winp6)).otherwise(col(\"Rainfall\")))\n",
    ".withColumn(\"Evaporation\",when(col(\"Evaporation\").isNull,avg(col(\"Evaporation\")).over(winp6)).otherwise(col(\"Evaporation\")))\n",
    ".withColumn(\"Sunshine\",when(col(\"Sunshine\").isNull,avg(col(\"Sunshine\")).over(winp6)).otherwise(col(\"Sunshine\")))\n",
    ".where(col(\"row\")===1).select(\"Date\",\"Month\",\"Location\",\"MinTemp\",\"MaxTemp\",\"Rainfall\",\"Evaporation\",\"Sunshine\",\"WindGustDir\",\n",
    "                              \"WindGustSpeed\",\"WindDir9am\",\"WindDir3pm\",\n",
    "                                    \"WindSpeed9am\",\"WindSpeed3pm\",\"Humidity9am\",\"Humidity3pm\",\"Pressure9am\",\"Pressure3pm\",\n",
    "                              \"Cloud9am\",\"Cloud3pm\",\"Temp9am\",\"Temp3pm\",\"RainToday\",\"RISK_MM\",\"RainTomorrow\")\n",
    "      .orderBy(\"Location\",\"Month\").toDF()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_org_df_ff.createOrReplaceTempView(\"Aus_Rain_Data_Final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------+-------+-------+--------+-----------------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+-------+------------+\n",
      "|      Date|Month|Location|MinTemp|MaxTemp|Rainfall|      Evaporation|Sunshine|WindGustDir|WindGustSpeed|WindDir9am|WindDir3pm|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|Cloud9am|Cloud3pm|Temp9am|Temp3pm|RainToday|RISK_MM|RainTomorrow|\n",
      "+----------+-----+--------+-------+-------+--------+-----------------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+-------+------------+\n",
      "|2009-01-01|    1|Adelaide|   16.9|   22.9|     0.0|             14.2|    10.9|         SW|         50.0|        SW|        SW|        17.0|        20.0|       58.0|       40.0|     1011.3|     1012.4|    null|    null|   18.1|   22.5|       No|    0.0|          No|\n",
      "|2009-02-01|    2|Adelaide|   25.9|   40.6|     0.0|             24.0|    10.0|         SE|         65.0|       SSE|       SSW|         7.0|        17.0|       41.0|       24.0|     1012.8|     1010.2|    null|    null|   29.9|   37.4|       No|    0.0|          No|\n",
      "|2009-03-01|    3|Adelaide|   15.2|   24.0|     0.0|             11.8|     5.2|          W|         28.0|         W|       WSW|         4.0|        17.0|       62.0|       46.0|     1015.7|     1014.4|    null|    null|   17.1|   21.7|       No|    0.0|          No|\n",
      "|2009-04-01|    4|Adelaide|   19.3|   34.1|     0.0|              7.0|    11.0|          N|         31.0|        NE|         N|        11.0|        15.0|       37.0|       18.0|     1017.8|     1015.6|    null|    null|   25.3|   33.0|       No|    0.0|          No|\n",
      "|2009-05-01|    5|Adelaide|    9.8|   17.4|     0.0|              1.4|     0.7|        SSE|         24.0|        NE|       SSE|         9.0|         2.0|       82.0|       91.0|     1024.0|     1022.5|    null|    null|   14.2|   15.2|       No|    8.6|         Yes|\n",
      "|2009-06-01|    6|Adelaide|    9.2|   16.6|     0.0|              1.6|     2.4|        ENE|         31.0|        SW|       SSE|         7.0|         7.0|       73.0|       60.0|     1026.6|     1024.2|    null|    null|   12.8|   15.3|       No|    0.0|          No|\n",
      "|2008-07-01|    7|Adelaide|    8.8|   15.7|     5.0|              1.6|     2.6|         NW|         48.0|        SW|         W|        13.0|        15.0|       92.0|       67.0|     1017.4|     1017.7|    null|    null|   13.5|   14.9|      Yes|    0.8|          No|\n",
      "|2008-08-03|    8|Adelaide|    7.2|   18.1|     8.2|              4.4|     5.6|        NNW|         39.0|       NNE|       NNW|        15.0|        19.0|       54.0|       37.0|     1017.3|     1012.4|    null|    null|   11.8|   17.8|       NA|    8.2|         Yes|\n",
      "|2008-09-01|    9|Adelaide|    6.9|   17.1|     0.8|              5.8|     2.8|         NW|         44.0|       NNW|       WNW|        17.0|        26.0|       76.0|       54.0|     1022.1|     1021.0|    null|    null|   10.5|   16.4|       No|    1.0|          No|\n",
      "|2008-10-01|   10|Adelaide|   15.0|   27.2|     0.0|              6.8|     7.7|        NNW|         65.0|       NNE|         W|        22.0|        24.0|       14.0|       48.0|     1006.9|     1007.1|    null|    null|   21.1|   23.7|       No|    0.0|          No|\n",
      "|2008-11-01|   11|Adelaide|   12.8|   26.2|     1.4|9.182038834951452|     6.2|          E|         39.0|         E|       ESE|        19.0|        11.0|       56.0|       34.0|     1022.3|     1018.1|    null|    null|   15.7|   23.8|       NA|    1.4|         Yes|\n",
      "|2008-12-01|   12|Adelaide|   12.6|   21.3|     0.0|              6.8|     9.6|        WSW|         33.0|       WSW|        SW|        20.0|        17.0|       46.0|       44.0|     1012.8|     1012.1|    null|    null|   17.1|   20.2|       No|    0.0|          No|\n",
      "|2009-01-01|    1|  Albany|   16.4|   21.4|     0.0|              8.0|     9.6|         NA|         null|         E|       ESE|        13.0|        31.0|       64.0|       64.0|     1017.9|     1015.6|     8.0|     1.0|   19.4|   20.8|       No|    0.0|          No|\n",
      "|2009-02-01|    2|  Albany|   20.2|   26.0|     0.0|              7.8|    10.1|         NA|         null|        NE|       ESE|         6.0|        24.0|       84.0|       74.0|     1014.9|     1012.7|     8.0|     1.0|   22.0|   24.4|       No|    0.0|          No|\n",
      "|2009-03-01|    3|  Albany|   18.5|   24.5|     5.4|              4.6|     3.4|         NA|         null|       ENE|         E|         7.0|        11.0|       87.0|       84.0|     1009.4|     1004.8|     6.0|     7.0|   20.6|   22.2|      Yes|   16.4|         Yes|\n",
      "|2009-04-01|    4|  Albany|   13.2|   16.9|     7.2|              2.0|     9.8|         NA|         null|       SSW|       SSW|        19.0|        13.0|       53.0|       52.0|     1024.1|     1023.5|     2.0|     1.0|   15.4|   16.0|      Yes|    0.0|          No|\n",
      "|2009-05-01|    5|  Albany|   13.5|   19.0|     0.0|              2.4|     2.8|         NA|         null|        NA|       SSW|         0.0|         7.0|       73.0|       71.0|     1030.4|     1028.7|     8.0|     6.0|   15.0|   18.0|       No|    0.0|          No|\n",
      "|2009-06-01|    6|  Albany|    9.6|   20.1|     0.8|              0.2|     7.1|         NA|         null|       NNE|       ESE|         6.0|        15.0|       92.0|       68.0|     1022.0|     1018.7|     7.0|     1.0|   11.5|   19.7|       No|    0.2|          No|\n",
      "|2009-07-01|    7|  Albany|    8.9|   14.8|    14.4|              1.4|     6.5|         NA|         null|       WSW|       WSW|        41.0|        33.0|       76.0|       57.0|     1017.5|     1020.5|     7.0|     5.0|   13.2|   14.5|      Yes|    0.0|          No|\n",
      "|2009-08-01|    8|  Albany|    7.2|   18.7|     0.0|              2.0|     9.9|         NA|         null|        NA|         N|         0.0|         6.0|       91.0|       51.0|     1024.2|     1021.1|     1.0|     3.0|    9.5|   18.1|       No|    0.0|          No|\n",
      "+----------+-----+--------+-------+-------+--------+-----------------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from Aus_Rain_Data_Final\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+-------+------------+\n",
      "|Date|Location|MinTemp|MaxTemp|Rainfall|Evaporation|Sunshine|WindGustDir|WindGustSpeed|WindDir9am|WindDir3pm|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|Cloud9am|Cloud3pm|Temp9am|Temp3pm|RainToday|RISK_MM|RainTomorrow|\n",
      "+----+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+-------+------------+\n",
      "|   0|       0|      0|      0|       0|        197|     228|          0|           24|         0|         0|           0|           0|          0|          0|         48|         48|     144|     144|      0|      0|        0|      0|           0|\n",
      "+----+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_org_df_ff.select(weather_org_df_cast.columns.map(colName => {\n",
    "    count(when(col(colName).isNull, true) || when(col(colName)===\"\", true)) as s\"${colName}\"\n",
    "  }): _*)\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manage to remove null values for most of the columns . For some of the columns Like sunshine and Evaporation missing values still exist as the values are missing for the entire month. We will handle this if necessary during the modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3  Removing Data set out side of the expected range ( Excluding Outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weather_org_df_ff_outl: org.apache.spark.sql.DataFrame = [Date: date, Month: int ... 23 more fields]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weather_org_df_ff_outl = weather_org_df_ff.withColumn(\"MinTemp\",when(col(\"MinTemp\")<(-10),null).otherwise(col(\"MinTemp\")))\n",
    "            .withColumn(\"MinTemp\",when(col(\"MinTemp\")>(50),null).otherwise(col(\"MinTemp\")))\n",
    "            .withColumn(\"MaxTemp\",when(col(\"MaxTemp\")<(-10),null).otherwise(col(\"MaxTemp\")))\n",
    "            .withColumn(\"MaxTemp\",when(col(\"MaxTemp\")>(50),null).otherwise(col(\"MaxTemp\")))\n",
    "            .withColumn(\"Temp9am\",when(col(\"Temp9am\")<(-10),null).otherwise(col(\"Temp9am\")))\n",
    "            .withColumn(\"Temp9am\",when(col(\"Temp9am\")>(50),null).otherwise(col(\"Temp9am\")))\n",
    "            .withColumn(\"Temp3pm\",when(col(\"Temp3pm\")<(-10),null).otherwise(col(\"Temp3pm\")))\n",
    "            .withColumn(\"Temp3pm\",when(col(\"Temp3pm\")>(50),null).otherwise(col(\"Temp3pm\")))\n",
    "            .withColumn(\"Cloud9am\",when(col(\"Cloud9am\")<(0),null).otherwise(col(\"Cloud9am\")))\n",
    "            .withColumn(\"Cloud9am\",when(col(\"Cloud9am\")>(9),null).otherwise(col(\"Cloud9am\")))\n",
    "            .withColumn(\"Cloud3pm\",when(col(\"Cloud3pm\")<(0),null).otherwise(col(\"Cloud3pm\")))\n",
    "            .withColumn(\"Cloud3pm\",when(col(\"Cloud3pm\")>(9),null).otherwise(col(\"Cloud3pm\")))\n",
    "            .withColumn(\"Humidity9am\",when(col(\"Humidity9am\")<(0),null).otherwise(col(\"Humidity9am\")))\n",
    "            .withColumn(\"Humidity9am\",when(col(\"Humidity9am\")>(100),null).otherwise(col(\"Humidity9am\")))\n",
    "            .withColumn(\"Humidity3pm\",when(col(\"Humidity3pm\")<(0),null).otherwise(col(\"Humidity3pm\")))\n",
    "            .withColumn(\"Humidity3pm\",when(col(\"Humidity3pm\")>(100),null).otherwise(col(\"Humidity3pm\")))\n",
    "            .withColumn(\"Pressure9am\",when(col(\"Pressure9am\")<(970),null).otherwise(col(\"Pressure9am\")))\n",
    "            .withColumn(\"Pressure9am\",when(col(\"Pressure9am\")>(1050),null).otherwise(col(\"Pressure9am\")))\n",
    "            .withColumn(\"Pressure3pm\",when(col(\"Pressure3pm\")<(970),null).otherwise(col(\"Pressure3pm\")))\n",
    "            .withColumn(\"Pressure3pm\",when(col(\"Pressure3pm\")>(1050),null).otherwise(col(\"Pressure3pm\")))\n",
    "            .withColumn(\"WindGustSpeed\",when(col(\"WindGustSpeed\")<(0),null).otherwise(col(\"WindGustSpeed\")))\n",
    "            .withColumn(\"WindGustSpeed\",when(col(\"WindGustSpeed\")>(150),null).otherwise(col(\"WindGustSpeed\")))\n",
    "            .withColumn(\"WindSpeed9am\",when(col(\"WindSpeed9am\")<(0),null).otherwise(col(\"WindSpeed9am\")))\n",
    "            .withColumn(\"WindSpeed9am\",when(col(\"WindSpeed9am\")>(150),null).otherwise(col(\"WindSpeed9am\")))\n",
    "            .withColumn(\"WindSpeed3pm\",when(col(\"WindSpeed3pm\")<(0),null).otherwise(col(\"WindSpeed3pm\")))\n",
    "            .withColumn(\"WindSpeed3pm\",when(col(\"WindSpeed3pm\")>(150),null).otherwise(col(\"WindSpeed3pm\")))\n",
    "            .withColumn(\"Rainfall\",when(col(\"Rainfall\")<(0),null).otherwise(col(\"Rainfall\")))\n",
    "            .withColumn(\"Rainfall\",when(col(\"Rainfall\")>(500),null).otherwise(col(\"Rainfall\")))\n",
    "            .withColumn(\"Evaporation\",when(col(\"Evaporation\")<(0),null).otherwise(col(\"Evaporation\")))\n",
    "            .withColumn(\"Evaporation\",when(col(\"Evaporation\")>(100),null).otherwise(col(\"Evaporation\")))\n",
    "            .withColumn(\"Sunshine\",when(col(\"Sunshine\")<(0),null).otherwise(col(\"Sunshine\")))\n",
    "            .withColumn(\"Sunshine\",when(col(\"Sunshine\")>(15),null).otherwise(col(\"Sunshine\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5 Scaling of the following columns in order to get correct results in Modeling \n",
    "\n",
    "( Note Yury code goes here )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5  Final out put cleaned file  - Save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Saving into Single partition\n",
    "weather_org_df_ff_outl.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"weather_s.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
