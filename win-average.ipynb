{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://ed10c28200f9:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1591191339096)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weather_org_df: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 30 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weather_org_df = spark.\n",
    "        read.\n",
    "        option(\"inferSchema\", \"true\").\n",
    "        option(\"header\",\"true\").\n",
    "        option(\"mode\",\"DROPMALFORMED\").\n",
    "        option(\"delimiter\", \",\").\n",
    "        csv(\"weatherAUS-clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-03 13:36:19,180 WARN  [Thread-4] util.Utils (Logging.scala:logWarning(66)) - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n"
     ]
    }
   ],
   "source": [
    "weather_org_df.registerTempTable(\"Aus_Rain_Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "/* val df0 = spark.sql(\"select * from Aus_Rain_Data\") */"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df0: org.apache.spark.sql.DataFrame = [Location: string, date_Sc: timestamp ... 1 more field]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df0 = spark.sql(\"select Location, date_Sc, MinTemp from Aus_Rain_Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Location: string (nullable = true)\n",
      " |-- date_Sc: timestamp (nullable = true)\n",
      " |-- MinTemp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df1: org.apache.spark.sql.DataFrame = [Location: string, MinTemp: string ... 1 more field]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1 = df0.withColumn(\"date\", df0(\"date_Sc\").cast(DateType))\n",
    "    .drop(\"date_Sc\")\n",
    "    .withColumnRenamed(\"date_Sc\", \"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [Location: string, date: date ... 1 more field]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = df1.withColumn(\"MinTemp_Sc\", df1(\"MinTemp\").cast(DoubleType))\n",
    "    .drop(\"MinTemp\")\n",
    "    .withColumnRenamed(\"MinTemp_Sc\", \"MinTemp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Location: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- MinTemp: double (nullable = true)\n",
      "\n",
      "+--------+----------+-------+\n",
      "|Location|      date|MinTemp|\n",
      "+--------+----------+-------+\n",
      "|  Albury|2008-12-01|   13.4|\n",
      "|  Albury|2008-12-02|    7.4|\n",
      "|  Albury|2008-12-03|   12.9|\n",
      "|  Albury|2008-12-04|    9.2|\n",
      "|  Albury|2008-12-05|   17.5|\n",
      "|  Albury|2008-12-06|   14.6|\n",
      "|  Albury|2008-12-07|   14.3|\n",
      "|  Albury|2008-12-08|    7.7|\n",
      "|  Albury|2008-12-09|    9.7|\n",
      "|  Albury|2008-12-10|   13.1|\n",
      "+--------+----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()\n",
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "window: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@1cf0b336\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val window = Window.partitionBy(\"Location\").orderBy(\"date\").rowsBetween(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colNames: List[String] = List(MinTemp)\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val colNames = List(\"MinTemp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df3: org.apache.spark.sql.DataFrame = [Location: string, date: date ... 2 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*for(column <- colNames){\n",
    "  val df3 = df2.withColumn(column+\"_Avg\", mean(df2(column)).over(window))\n",
    "} */\n",
    "\n",
    " val df3 = df2.withColumn(\"MinTemp_Avg\", mean(df2(\"MinTemp\")).over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+------------------+\n",
      "|Location|      date|MinTemp|       MinTemp_Avg|\n",
      "+--------+----------+-------+------------------+\n",
      "|  Cairns|2008-12-01|   25.2|              24.7|\n",
      "|  Cairns|2008-12-02|   24.2|24.433333333333334|\n",
      "|  Cairns|2008-12-03|   23.9|23.433333333333334|\n",
      "|  Cairns|2008-12-04|   22.2|23.099999999999998|\n",
      "|  Cairns|2008-12-05|   23.2|23.366666666666664|\n",
      "|  Cairns|2008-12-06|   24.7|23.933333333333334|\n",
      "|  Cairns|2008-12-07|   23.9|25.066666666666663|\n",
      "|  Cairns|2008-12-08|   26.6| 25.53333333333333|\n",
      "|  Cairns|2008-12-09|   26.1|26.233333333333334|\n",
      "|  Cairns|2008-12-10|   26.0|              25.8|\n",
      "+--------+----------+-------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "/* if (a == b) {\n",
    "    doSomething()\n",
    "} else {\n",
    "    doSomethingElse()\n",
    "} */\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.coalesce(1)\n",
    "      .write\n",
    "      .option(\"header\",\"true\")\n",
    "      .option(\"sep\",\",\")\n",
    "      .mode(\"overwrite\")\n",
    "      .csv(\"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS  part-00000-ea9b548f-bf69-4c90-bc02-3af6a6ebd8f1-c000.csv\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ls /home/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv /home/testing/*.csv /home/testing.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Permanently added '10.0.2.15' (ECDSA) to the list of known hosts.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ssh yury@10.0.2.15 \"docker cp bd_cont:/home/testing.csv /home/yury/Downloads/FIT5202-Project/big_data_tutorials/testing.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
