{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://0a53d4451f83:4041\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1591425630672)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-06 06:40:29,844 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2020-06-06 06:40:30,596 WARN  [Thread-4] util.Utils (Logging.scala:logWarning(66)) - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import spark.implicits._\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql.SQLContext\n",
       "import org.apache.spark.{SparkContext, SparkConf}\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.ml.linalg.{Matrix, Vectors}\n",
       "import org.apache.spark.ml.stat.Correlation\n",
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.sql.{Row, SparkSession}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._ \n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.{SparkContext, SparkConf}\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.linalg.{Matrix, Vectors}\n",
    "import org.apache.spark.ml.stat.Correlation\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.{Row, SparkSession}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Date: timestamp, Location: string ... 31 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.\n",
    "        read.\n",
    "        option(\"inferSchema\", \"true\").\n",
    "        option(\"header\",\"true\").\n",
    "        option(\"mode\",\"DROPMALFORMED\").\n",
    "        option(\"delimiter\", \",\").\n",
    "        csv(\"weatherAUS-clean-FINAL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MinTemp: double (nullable = true)\n",
      " |-- MaxTemp: double (nullable = true)\n",
      " |-- Rainfall: double (nullable = true)\n",
      " |-- WindSpeed9am: double (nullable = true)\n",
      " |-- WindSpeed3pm: double (nullable = true)\n",
      " |-- Humidity9am: double (nullable = true)\n",
      " |-- Humidity3pm: double (nullable = true)\n",
      " |-- Temp9am: double (nullable = true)\n",
      " |-- Temp3pm: double (nullable = true)\n",
      " |-- RainToday_Sc: double (nullable = true)\n",
      " |-- RainTomorrow_Sc: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_num: org.apache.spark.sql.DataFrame = [MinTemp: double, MaxTemp: double ... 9 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_num = df.drop(\"Date\",\"Location\",\"WindGustDir\",\"WindDir9am\",\"WindDir3pm\",\"RainToday\",\"RainTomorrow\",\n",
    "                     \"RISK_MM\",\"Month\",\"Pressure9am\",\"Pressure3pm\",\"date_Sc\",\"date-day_Sc\",\"date-month_Sc\",\"date-year_Sc\",\n",
    "                     \"Evaporation\",\"Sunshine\",\"Cloud9am\",\"Cloud3pm\",\"Pressure3pm_Sc\",\"Pressure9am_Sc\",\"WindGustSpeed\")\n",
    "                .withColumn(\"RainToday_Sc\",col(\"RainToday_Sc\").cast(DoubleType))\n",
    "                .withColumn(\"RainTomorrow_Sc\",col(\"RainTomorrow_Sc\").cast(DoubleType))\n",
    "df_num.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-06 06:40:48,633 WARN  [Executor task launch worker for task 6] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2020-06-06 06:40:48,634 WARN  [Executor task launch worker for task 6] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "vector_col: String = corr_features\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_86e749d0d66f\n",
       "df_vector: org.apache.spark.sql.DataFrame = [corr_features: vector]\n",
       "coeff1: org.apache.spark.ml.linalg.Matrix =\n",
       "1.0                    0.736773319224934     ... (11 total)\n",
       "0.736773319224934      1.0                   ...\n",
       "0.1032747100008999     -0.07375231160514448  ...\n",
       "0.17361779475707115    0.01309910338554134   ...\n",
       "0.17271467643471622    0.04975583636448577   ...\n",
       "-0.23576460688378328   -0.5044486150424999   ...\n",
       "1.6918117304553726E-4  -0.5086602899657561   ...\n",
       "0.9019093470536083     0.8876454668089747    ...\n",
       "0.7123737658823907     0.9828715270649254    ...\n",
       "0.05525968787984383    -0.22668484736527175  ...\n",
       "0.08336903898009859    -0.15945177018161483  ...\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// convert to vector column first\n",
    "val vector_col = \"corr_features\"\n",
    "val assembler = new VectorAssembler().setInputCols(df_num.columns.toArray).setOutputCol(vector_col)\n",
    "val df_vector = assembler.transform(df_num).select(vector_col)\n",
    "\n",
    "// get correlation matrix\n",
    "val Row(coeff1: Matrix) = Correlation.corr(df_vector,vector_col).head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cols: scala.collection.immutable.Range = Range(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
       "col_names: Array[String] = Array(MinTemp, MaxTemp, Rainfall, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Temp9am, Temp3pm, RainToday_Sc, RainTomorrow_Sc)\n",
       "df: org.apache.spark.sql.DataFrame = [arr: array<double>]\n",
       "df2: org.apache.spark.sql.DataFrame = [_1: double, _2: double ... 9 more fields]\n",
       "df3: org.apache.spark.sql.DataFrame = [_1: double, _2: double ... 9 more fields]\n",
       "df_CM: org.apache.spark.sql.DataFrame = [MinTemp: double, MaxTemp: double ... 9 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cols = (0 until coeff1.numCols).toSeq\n",
    "val col_names = df_num.columns.toArray\n",
    "\n",
    "val df = coeff1.transpose\n",
    "  .colIter.toSeq\n",
    "  .map(_.toArray)\n",
    "  .toDF(\"arr\")\n",
    "\n",
    "val df2 = cols.foldLeft(df)((df, i) => df.withColumn(\"_\" + (i+1), $\"arr\"(i)))\n",
    "  .drop(\"arr\")\n",
    "  \n",
    "val df3 = df2.columns.foldLeft(df2){(df,colName) =>df.withColumn(colName,round(col(colName),3))}\n",
    "\n",
    "val df_CM = df3.toDF(col_names:_*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+------------+------------+-----------+-----------+-------+-------+------------+---------------+\n",
      "|MinTemp|MaxTemp|Rainfall|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Temp9am|Temp3pm|RainToday_Sc|RainTomorrow_Sc|\n",
      "+-------+-------+--------+------------+------------+-----------+-----------+-------+-------+------------+---------------+\n",
      "|    1.0|  0.737|   0.103|       0.174|       0.173|     -0.236|        0.0|  0.902|  0.712|       0.055|          0.083|\n",
      "|  0.737|    1.0|  -0.074|       0.013|        0.05|     -0.504|     -0.509|  0.888|  0.983|      -0.227|         -0.159|\n",
      "|  0.103| -0.074|     1.0|       0.088|       0.057|      0.222|      0.251|   0.01| -0.076|       0.487|          0.245|\n",
      "|  0.174|  0.013|   0.088|         1.0|       0.516|      -0.27|     -0.031|  0.126| -0.001|       0.101|          0.091|\n",
      "|  0.173|   0.05|   0.057|       0.516|         1.0|     -0.147|      0.015|  0.162|  0.025|       0.077|          0.086|\n",
      "| -0.236| -0.504|   0.222|       -0.27|      -0.147|        1.0|      0.665| -0.473| -0.495|       0.349|          0.257|\n",
      "|    0.0| -0.509|   0.251|      -0.031|       0.015|      0.665|        1.0| -0.228| -0.558|       0.373|          0.443|\n",
      "|  0.902|  0.888|    0.01|       0.126|       0.162|     -0.473|     -0.228|    1.0|  0.863|      -0.098|         -0.028|\n",
      "|  0.712|  0.983|  -0.076|      -0.001|       0.025|     -0.495|     -0.558|  0.863|    1.0|      -0.231|         -0.192|\n",
      "|  0.055| -0.227|   0.487|       0.101|       0.077|      0.349|      0.373| -0.098| -0.231|         1.0|          0.307|\n",
      "|  0.083| -0.159|   0.245|       0.091|       0.086|      0.257|      0.443| -0.028| -0.192|       0.307|            1.0|\n",
      "+-------+-------+--------+------------+------------+-----------+-----------+-------+-------+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_CM.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
